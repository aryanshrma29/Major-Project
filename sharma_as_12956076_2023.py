# -*- coding: utf-8 -*-
"""Sharma_AS_12956076_2023.ipynb

Automatically generated by Colaboratory.



import pandas as pd

data = pd.read_csv('/content/in-vehicle-coupon-recommendation (1).csv')

data

data = data.drop('car', axis=1)

data

data = data.dropna()

data

# feature encoding
from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder()
categorical_feature = ['destination','passanger','weather','time','coupon','expiration','gender','maritalStatus','education','occupation','income','Bar','CoffeeHouse',
                       'CarryAway','RestaurantLessThan20','Restaurant20To50']

data[categorical_feature] = encoder.fit_transform(data[categorical_feature])

#to know which encoded value corresponds to which label (mapper)
encoded_data_mapping = {}
for feature in categorical_feature:
    encoded_data_mapping[feature] = dict(enumerate(encoder.categories_[categorical_feature.index(feature)]))

data

encoded_data_mapping

data['age'] = data['age'].replace('50plus', '50')
data['age'] = data['age'].replace('below21', '15')

data





output = data['Y']
input_features = data.drop('Y', axis=1)

########################################## FEATURE SELECTION RFE ########################################################################

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
n_features = 15
selector = RFE(estimator=model, n_features_to_select=n_features)
X_new = selector.fit_transform(input_features, output)
selected_feature_indices = selector.get_support(indices=True)

selected_feature_indices

selected_data = input_features.iloc[:, selected_feature_indices]
selected_data

X = selected_data
y = output

#Splitting the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.025, random_state=42)

X_train

X_test

!pip install shap



####################  xgboost ###################
############################################################

import xgboost as xgb
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

from sklearn.metrics import classification_report, accuracy_score
y_pred = model.predict(X_test)
from sklearn.model_selection import KFold, cross_val_score
scores = cross_val_score(model, X_test, y_test, cv = KFold(n_splits = 5))
import numpy as np
print("cross-validation score:",np.mean(scores),'\n')
print(classification_report(y_test, y_pred))

#Getting shap values
import shap
explainer = shap.Explainer(model, X_train)
shap_values = explainer.shap_values(X_test)

shap_values.shape

instance= 55
shap.waterfall_plot(shap.Explanation(values=shap_values[instance], base_values=explainer.expected_value, data=X_test.iloc[instance]), max_display=15)

instance = 55
shap.initjs()
shap.plots.force(shap.Explanation(values=shap_values[instance], base_values=explainer.expected_value, data=X_test.iloc[instance]))

shap.initjs()
shap.plots.force(shap.Explanation(values=shap_values[0:100], base_values=explainer.expected_value, data=X_test.iloc[0:100]))

instance = 55
shap.plots.bar(shap.Explanation(values=shap_values[instance], base_values=explainer.expected_value, data=X_test.iloc[instance]), max_display=15)

shap.summary_plot(shap_values, X_test, plot_type='bar')



#### LIME ####

!pip install lime

import lime
import lime.lime_tabular
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['0', '1'], discretize_continuous=False)
instance = X_test.iloc[[55]].values[0]

explanation = explainer.explain_instance(instance, model.predict_proba, num_features=5)
explanation.show_in_notebook(show_table=True)





####################  RandomForestClassifier ###################
############################################################

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 500, random_state = 42)
model = rf.fit(X_train, y_train)

from sklearn.metrics import classification_report
y_pred = model.predict(X_test)
from sklearn.model_selection import KFold, cross_val_score
scores = cross_val_score(model, X_test, y_test, cv = KFold(n_splits = 5))
import numpy as np
print("cross-validation score:",np.mean(scores),'\n')
print(classification_report(y_test,y_pred))

############### SHAP IMPLEMENTATION ####################

import shap
explainer = shap.TreeExplainer(model)

instance = 24
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.plots.waterfall(shap.Explanation(values=shap_values[1], base_values=explainer.expected_value[1], data=X_test.iloc[instance]),max_display=15)

instance = 24
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.initjs()
shap.plots.force(shap.Explanation(values=shap_values[1], base_values=explainer.expected_value[1], data=X_test.iloc[instance]))

instance = 24
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.plots.bar(shap.Explanation(values=shap_values[1], base_values=explainer.expected_value[1], data=X_test.iloc[instance]))

subset = X_test[:25]
shap_values = explainer.shap_values(subset)
shap.summary_plot(shap_values, subset, plot_type='bar')

############### LIME IMPLEMENTATION ####################

import lime
import lime.lime_tabular
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['0', '1'], discretize_continuous=False)
instance = X_test.iloc[[24]].values[0]

explanation = explainer.explain_instance(instance, model.predict_proba, num_features=5)
explanation.show_in_notebook(show_table=True)





#################### LOGISTIC REGRESSION ###################
############################################################

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

from sklearn.metrics import classification_report
y_pred = model.predict(X_test)
from sklearn.model_selection import KFold, cross_val_score
scores = cross_val_score(model, X_test, y_test, cv = KFold(n_splits = 5))
import numpy as np
print("cross-validation score:",np.mean(scores),'\n')
print(classification_report(y_test,y_pred))

############### SHAP IMPLEMENTATION ####################

explainer = shap.LinearExplainer(model, X_train)

instance = 15
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.plots.waterfall(shap.Explanation(values=shap_values, base_values=explainer.expected_value, data=X_test.iloc[instance]),max_display=15)

instance = 15
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.initjs()
shap.plots.force(shap.Explanation(values=shap_values, base_values=explainer.expected_value, data=X_test.iloc[instance]))

instance = 15
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.plots.bar(shap.Explanation(values=shap_values, base_values=explainer.expected_value, data=X_test.iloc[instance]),max_display=15)

subset = X_test[0:25]
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test,  plot_type="bar")

############### LIME IMPLEMENTATION ####################

import lime
import lime.lime_tabular
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['0', '1'], discretize_continuous=False)
instance = X_test.iloc[[15]].values[0]

explanation = explainer.explain_instance(instance, model.predict_proba, num_features=5)
explanation.show_in_notebook(show_table=True)





#################### Decision tree ###################
######################################################

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

from sklearn.metrics import classification_report
y_pred = model.predict(X_test)
from sklearn.model_selection import KFold, cross_val_score
scores = cross_val_score(model, X_test, y_test, cv = KFold(n_splits = 5))
import numpy as np
print("cross-validation score:",np.mean(scores),'\n')
print(classification_report(y_test,y_pred))

############### SHAP IMPLEMENTATION ####################

import shap
explainer = shap.TreeExplainer(model)

instance = 30
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.plots.waterfall(shap.Explanation(values=shap_values[1], base_values=explainer.expected_value[1], data=X_test.iloc[instance]),max_display=15)

instance = 30
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.initjs()
shap.plots.force(shap.Explanation(values=shap_values[1], base_values=explainer.expected_value[1], data=X_test.iloc[instance]))

instance = 30
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.plots.bar(shap.Explanation(values=shap_values[1], base_values=explainer.expected_value[1], data=X_test.iloc[instance]))

subset = X_test[0:25]
shap_values = explainer.shap_values(subset)
shap.summary_plot(shap_values, subset)

############### LIME IMPLEMENTATION ####################

import lime
import lime.lime_tabular
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['0', '1'], discretize_continuous=False)
instance = X_test.iloc[[30]].values[0]

explanation = explainer.explain_instance(instance, model.predict_proba, num_features=5)
explanation.show_in_notebook(show_table=True)





#################### SVC ###################
######################################################

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
model = SVC(kernel='linear', probability=True, random_state=42)
model.fit(X_train, y_train)

from sklearn.model_selection import KFold, cross_val_score
y_pred = model.predict(X_test)
scores = cross_val_score(model, X_test, y_test, cv = KFold(n_splits = 5))
import numpy as np
print("cross-validation score:",np.mean(scores),'\n')
from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

############### SHAP IMPLEMENTATION ####################

import shap
subset = X_train[:100]
explainer = shap.KernelExplainer(model.predict_proba, subset)

instance = 35
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.plots.waterfall(shap.Explanation(values=shap_values[1], base_values=explainer.expected_value[1], data=X_test.iloc[instance]), max_display=15)

instance = 35
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.initjs()
shap.plots.force(shap.Explanation(values=shap_values[1], base_values=explainer.expected_value[1], data=X_test.iloc[instance]))

instance = 35
shap_values = explainer.shap_values(X_test.iloc[instance])
shap.plots.bar(shap.Explanation(values=shap_values[1], base_values=explainer.expected_value[1], data=X_test.iloc[instance]),max_display=15)

subset = X_test[:15]
shap_values = explainer.shap_values(subset)
shap.summary_plot(shap_values, X_test)

############### LIME IMPLEMENTATION ####################

import lime
import lime.lime_tabular
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['0', '1'], discretize_continuous=False)
instance = X_test.iloc[[35]].values[0]

explanation = explainer.explain_instance(instance, model.predict_proba, num_features=5)
explanation.show_in_notebook(show_table=True)



















